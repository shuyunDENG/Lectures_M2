# 机器学习核心算法详细数学推导

本文档包含三个最重要算法的完整数学推导，每一步都清晰标注。

---

## 推导 1: AdaBoost 权重更新的理论推导

### 1.1 问题设定

**给定**：
- 训练数据集：{(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}，其中 yᵢ ∈ {-1, +1}
- 弱分类器序列：h₁(x), h₂(x), ..., hₜ(x)，每个满足 hₜ(x) ∈ {-1, +1}
- 数据点权重分布：Dₜ(i) 表示在第 t 轮时数据点 i 的权重

**目标**：找到权重 α₁, α₂, ..., αₜ 和更新规则，使得最终分类器
```
H(x) = sign(Σₜ αₜ·hₜ(x))
```
的误差最小。

### 1.2 指数损失函数

AdaBoost 使用**指数损失函数**：
```
L = Σᵢ₌₁ⁿ exp(-yᵢ·f(xᵢ))
```

其中 f(x) = Σₜ αₜ·hₜ(x) 是当前的组合分类器（不取sign，是实数值）。

**为什么用指数损失？**
- 可微分，便于优化
- 对误分类点施加指数级惩罚
- 数学上易处理

### 1.3 前向逐步加法模型

AdaBoost 采用**前向逐步加法**策略：每次只优化一个新的弱分类器。

假设已经得到了前 t-1 个分类器的组合：
```
fₜ₋₁(x) = Σₛ₌₁ᵗ⁻¹ αₛ·hₛ(x)
```

现在要加入第 t 个分类器：
```
fₜ(x) = fₜ₋₁(x) + αₜ·hₜ(x)
```

**第 t 步的损失函数**：
```
L = Σᵢ₌₁ⁿ exp(-yᵢ·fₜ(xᵢ))
  = Σᵢ₌₁ⁿ exp(-yᵢ·[fₜ₋₁(xᵢ) + αₜ·hₜ(xᵢ)])
  = Σᵢ₌₁ⁿ exp(-yᵢ·fₜ₋₁(xᵢ)) · exp(-yᵢ·αₜ·hₜ(xᵢ))
```

**定义权重**：
```
wᵢ⁽ᵗ⁾ = exp(-yᵢ·fₜ₋₁(xᵢ))
```

这个 wᵢ⁽ᵗ⁾ 是**固定的**（因为 fₜ₋₁ 已经确定），它就是数据点 i 在第 t 轮的重要性权重。

所以损失函数变为：
```
L = Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾ · exp(-yᵢ·αₜ·hₜ(xᵢ))
```

### 1.4 关键观察：yᵢ·hₜ(xᵢ) 只有两个值

因为 yᵢ, hₜ(xᵢ) ∈ {-1, +1}：

- **如果分类正确**：yᵢ = hₜ(xᵢ)，则 yᵢ·hₜ(xᵢ) = +1
- **如果分类错误**：yᵢ ≠ hₜ(xᵢ)，则 yᵢ·hₜ(xᵢ) = -1

**定义指示函数**：
```
Iᵢ = {1  如果 yᵢ ≠ hₜ(xᵢ)  (分类错误)
     {0  如果 yᵢ = hₜ(xᵢ)  (分类正确)
```

那么：
```
yᵢ·hₜ(xᵢ) = 1 - 2Iᵢ
```

**验证**：
- Iᵢ = 0 时：yᵢ·hₜ(xᵢ) = 1 - 0 = 1 ✓（正确）
- Iᵢ = 1 时：yᵢ·hₜ(xᵢ) = 1 - 2 = -1 ✓（错误）

### 1.5 重写损失函数

将 yᵢ·hₜ(xᵢ) = 1 - 2Iᵢ 代入：

```
L = Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾ · exp(-αₜ·(1 - 2Iᵢ))
  = Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾ · exp(-αₜ + 2αₜ·Iᵢ)
  = Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾ · exp(-αₜ) · exp(2αₜ·Iᵢ)
  = exp(-αₜ) · Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾ · exp(2αₜ·Iᵢ)
```

**分离正确和错误的样本**：
```
L = exp(-αₜ) · [Σ_{i:Iᵢ=0} wᵢ⁽ᵗ⁾ · exp(0) + Σ_{i:Iᵢ=1} wᵢ⁽ᵗ⁾ · exp(2αₜ)]
  = exp(-αₜ) · [Σ_{i:正确} wᵢ⁽ᵗ⁾ + exp(2αₜ) · Σ_{i:错误} wᵢ⁽ᵗ⁾]
```

**定义加权错误率**：
```
εₜ = Σ_{i:错误} wᵢ⁽ᵗ⁾ / Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾
```

**归一化权重**：设 W = Σᵢ₌₁ⁿ wᵢ⁽ᵗ⁾，则：
```
Σ_{i:正确} wᵢ⁽ᵗ⁾ = W - Σ_{i:错误} wᵢ⁽ᵗ⁾ = W(1 - εₜ)
Σ_{i:错误} wᵢ⁽ᵗ⁾ = W·εₜ
```

代入损失函数：
```
L = exp(-αₜ) · W · [(1 - εₜ) + exp(2αₜ)·εₜ]
  = W · exp(-αₜ) · [(1 - εₜ) + exp(2αₜ)·εₜ]
```

### 1.6 优化 αₜ

对 αₜ 求导，令导数为 0：
```
∂L/∂αₜ = W · [∂/∂αₜ(exp(-αₜ)·(1-εₜ)) + ∂/∂αₜ(exp(-αₜ)·exp(2αₜ)·εₜ)]
        = W · [-exp(-αₜ)·(1-εₜ) + exp(αₜ)·εₜ]
        = W · exp(-αₜ) · [-(1-εₜ) + exp(2αₜ)·εₜ]
```

令 ∂L/∂αₜ = 0：
```
-(1-εₜ) + exp(2αₜ)·εₜ = 0
exp(2αₜ)·εₜ = 1-εₜ
exp(2αₜ) = (1-εₜ)/εₜ
2αₜ = log((1-εₜ)/εₜ)
αₜ = ½·log((1-εₜ)/εₜ)  ← 这就是 αₜ 的公式！
```

### 1.7 权重更新推导

现在推导为什么权重更新是 Dₜ₊₁(i) ∝ Dₜ(i)·exp(...) 的形式。

**回忆**：我们定义的 wᵢ⁽ᵗ⁾ = exp(-yᵢ·fₜ₋₁(xᵢ))

在第 t+1 轮：
```
wᵢ⁽ᵗ⁺¹⁾ = exp(-yᵢ·fₜ(xᵢ))
         = exp(-yᵢ·[fₜ₋₁(xᵢ) + αₜ·hₜ(xᵢ)])
         = exp(-yᵢ·fₜ₋₁(xᵢ)) · exp(-yᵢ·αₜ·hₜ(xᵢ))
         = wᵢ⁽ᵗ⁾ · exp(-yᵢ·αₜ·hₜ(xᵢ))
```

**再次使用 yᵢ·hₜ(xᵢ) = 1 - 2Iᵢ**：
```
wᵢ⁽ᵗ⁺¹⁾ = wᵢ⁽ᵗ⁾ · exp(-αₜ·(1-2Iᵢ))
         = wᵢ⁽ᵗ⁾ · exp(-αₜ + 2αₜ·Iᵢ)
         = wᵢ⁽ᵗ⁾ · exp(-αₜ) · exp(2αₜ·Iᵢ)
```

**分两种情况**：

**情况 1：分类正确** (Iᵢ = 0)
```
wᵢ⁽ᵗ⁺¹⁾ = wᵢ⁽ᵗ⁾ · exp(-αₜ) · exp(0)
         = wᵢ⁽ᵗ⁾ · exp(-αₜ)
```

**情况 2：分类错误** (Iᵢ = 1)
```
wᵢ⁽ᵗ⁺¹⁾ = wᵢ⁽ᵗ⁾ · exp(-αₜ) · exp(2αₜ)
         = wᵢ⁽ᵗ⁾ · exp(αₜ)
```

**两者的比值**（这是关键！）：
```
错误权重的放大倍数 / 正确权重的放大倍数 = exp(αₜ) / exp(-αₜ) = exp(2αₜ)
```

代入 αₜ = ½·log((1-εₜ)/εₜ)：
```
exp(2αₜ) = exp(log((1-εₜ)/εₜ)) = (1-εₜ)/εₜ
```

### 1.8 定义 βₜ

AdaBoost 算法定义：
```
βₜ = εₜ/(1-εₜ)
```

那么：
```
1/βₜ = (1-εₜ)/εₜ = exp(2αₜ)
```

所以：
```
αₜ = ½·log(1/βₜ)  ← 这是 αₜ 用 βₜ 表示的形式
```

**权重更新规则**：

- **正确分类**：wᵢ⁽ᵗ⁺¹⁾ = wᵢ⁽ᵗ⁾ · exp(-αₜ) = wᵢ⁽ᵗ⁾ · exp(-½·log(1/βₜ)) = wᵢ⁽ᵗ⁾ · √βₜ

- **错误分类**：wᵢ⁽ᵗ⁺¹⁾ = wᵢ⁽ᵗ⁾ · exp(αₜ) = wᵢ⁽ᵗ⁾ · exp(½·log(1/βₜ)) = wᵢ⁽ᵗ⁾ / √βₜ

因为 βₜ < 1（对于弱分类器 εₜ < 0.5），所以：
- 正确的点权重减小（乘以 √βₜ < 1）
- 错误的点权重增大（除以 √βₜ，即乘以 1/√βₜ > 1）

**标准化后的权重分布** Dₜ(i) = wᵢ⁽ᵗ⁾ / Σⱼwⱼ⁽ᵗ⁾，AdaBoost 常写成：
```
Dₜ₊₁(i) ∝ Dₜ(i) · {βₜ      如果正确
                    {1/βₜ    如果错误

或者等价地：
Dₜ₊₁(i) ∝ Dₜ(i) · {βₜ          如果 hₜ(xᵢ) = yᵢ
                    {1          如果 hₜ(xᵢ) ≠ yᵢ
```

（这个等价形式更常见，因为归一化时常数可以吸收）

### 1.9 总结 AdaBoost 公式推导

| 公式 | 推导来源 |
|------|---------|
| **αₜ = ½·log((1-εₜ)/εₜ)** | 最小化指数损失对 αₜ 求导 |
| **βₜ = εₜ/(1-εₜ)** | 定义，简化表达 |
| **αₜ = ½·log(1/βₜ)** | 从上两式直接得出 |
| **Dₜ₊₁(i) ∝ Dₜ(i)·exp(-yᵢ·αₜ·hₜ(xᵢ))** | 指数损失的自然梯度 |
| **正确: Dₜ₊₁(i) ∝ Dₜ(i)·βₜ** | exp(-αₜ) = √βₜ |
| **错误: Dₜ₊₁(i) ∝ Dₜ(i)/βₜ** | exp(αₜ) = 1/√βₜ |

**核心思想**：
- 错误样本权重增加 1/βₜ 倍
- 正确样本权重减少到 βₜ 倍
- βₜ < 1 确保了错误样本在下一轮得到更多关注

---

## 推导 2: Logistic Regression 的 Hessian 矩阵

### 2.1 问题设定

**模型**：
```
P(y=1|x) = σ(β'x) = 1/(1 + exp(-β'x))
P(y=0|x) = 1 - σ(β'x) = 1/(1 + exp(β'x))
```

其中 β = (β₁, β₂, ..., βₚ)' 是参数向量，x = (x₁, x₂, ..., xₚ)' 是特征向量。

**为了简化，我们把截距吸收进去**：定义增广向量
```
β⁺ = (β₀, β₁, ..., βₚ)'  (p+1 维)
x⁺ = (1, x₁, ..., xₚ)'    (p+1 维)
```

这样 β'x + β₀ 就写成 β⁺'x⁺。

**数据集**：{(x₁, y₁), ..., (xₙ, yₙ)}，其中 yᵢ ∈ {0, 1}

### 2.2 对数似然函数

每个样本的似然：
```
P(yᵢ|xᵢ; β⁺) = [σ(β⁺'xᵢ⁺)]^yᵢ · [1-σ(β⁺'xᵢ⁺)]^(1-yᵢ)
```

**对数似然**：
```
log P(yᵢ|xᵢ; β⁺) = yᵢ·log(σ(β⁺'xᵢ⁺)) + (1-yᵢ)·log(1-σ(β⁺'xᵢ⁺))
```

**引入记号** zᵢ = β⁺'xᵢ⁺，pᵢ = σ(zᵢ) = 1/(1+exp(-zᵢ))：

```
log P(yᵢ|xᵢ; β⁺) = yᵢ·log(pᵢ) + (1-yᵢ)·log(1-pᵢ)
```

**总对数似然**（Log-Likelihood, LL）：
```
LL(β⁺) = Σᵢ₌₁ⁿ [yᵢ·log(pᵢ) + (1-yᵢ)·log(1-pᵢ)]
```

我们要**最大化** LL(β⁺)。

### 2.3 Sigmoid 函数的性质

**定义**：
```
σ(z) = 1/(1 + exp(-z)) = exp(z)/(1 + exp(z))
```

**重要性质 1**：
```
1 - σ(z) = 1 - 1/(1+exp(-z))
         = (1+exp(-z) - 1)/(1+exp(-z))
         = exp(-z)/(1+exp(-z))
         = 1/(1+exp(z))
         = σ(-z)
```

**重要性质 2**（求导）：
```
dσ(z)/dz = d/dz[1/(1+exp(-z))]
         = -1/(1+exp(-z))² · (-exp(-z))
         = exp(-z)/(1+exp(-z))²
         = [1/(1+exp(-z))] · [exp(-z)/(1+exp(-z))]
         = σ(z) · (1-σ(z))
         = σ(z) · σ(-z)
```

这个导数公式非常重要！

### 2.4 对数似然的梯度（一阶导数）

我们要计算 ∂LL/∂βⱼ⁺（对β⁺的第 j 个分量求导）。

**链式法则**：
```
∂LL/∂βⱼ⁺ = Σᵢ ∂/∂βⱼ⁺[yᵢ·log(pᵢ) + (1-yᵢ)·log(1-pᵢ)]
```

对第 i 个样本：
```
∂/∂βⱼ⁺[yᵢ·log(pᵢ) + (1-yᵢ)·log(1-pᵢ)]
= yᵢ · (1/pᵢ) · ∂pᵢ/∂βⱼ⁺ + (1-yᵢ) · (1/(1-pᵢ)) · ∂(1-pᵢ)/∂βⱼ⁺
= yᵢ · (1/pᵢ) · ∂pᵢ/∂βⱼ⁺ - (1-yᵢ) · (1/(1-pᵢ)) · ∂pᵢ/∂βⱼ⁺
= [yᵢ/pᵢ - (1-yᵢ)/(1-pᵢ)] · ∂pᵢ/∂βⱼ⁺
```

**计算 ∂pᵢ/∂βⱼ⁺**：

因为 pᵢ = σ(zᵢ) 且 zᵢ = β⁺'xᵢ⁺ = Σₖ βₖ⁺·xᵢₖ⁺：
```
∂pᵢ/∂βⱼ⁺ = ∂σ(zᵢ)/∂zᵢ · ∂zᵢ/∂βⱼ⁺
         = σ(zᵢ)·(1-σ(zᵢ)) · xᵢⱼ⁺     （用性质2）
         = pᵢ·(1-pᵢ) · xᵢⱼ⁺
```

**代入**：
```
[yᵢ/pᵢ - (1-yᵢ)/(1-pᵢ)] · pᵢ·(1-pᵢ) · xᵢⱼ⁺
= [yᵢ·(1-pᵢ) - (1-yᵢ)·pᵢ] · xᵢⱼ⁺
= [yᵢ - yᵢ·pᵢ - pᵢ + yᵢ·pᵢ] · xᵢⱼ⁺
= (yᵢ - pᵢ) · xᵢⱼ⁺
```

**梯度向量**：
```
∂LL/∂β⁺ = Σᵢ₌₁ⁿ (yᵢ - pᵢ) · xᵢ⁺
        = X⁺' · (y - p)
```

其中：
- X⁺ 是 n×(p+1) 的数据矩阵，第 i 行是 xᵢ⁺'
- y = (y₁, ..., yₙ)' 是标签向量
- p = (p₁, ..., pₙ)' 是预测概率向量

### 2.5 Hessian 矩阵（二阶导数）

Hessian 矩阵的第 (j,k) 元素是：
```
Hⱼₖ = ∂²LL/(∂βⱼ⁺∂βₖ⁺)
```

**从梯度出发**：
```
∂LL/∂βⱼ⁺ = Σᵢ₌₁ⁿ (yᵢ - pᵢ) · xᵢⱼ⁺
```

**对 βₖ⁺ 再求导**：
```
∂²LL/(∂βⱼ⁺∂βₖ⁺) = Σᵢ₌₁ⁿ ∂/∂βₖ⁺[(yᵢ - pᵢ) · xᵢⱼ⁺]
                  = Σᵢ₌₁ⁿ xᵢⱼ⁺ · ∂/∂βₖ⁺(yᵢ - pᵢ)     (xᵢⱼ⁺不依赖βₖ⁺)
                  = -Σᵢ₌₁ⁿ xᵢⱼ⁺ · ∂pᵢ/∂βₖ⁺           (yᵢ是常数)
```

**代入 ∂pᵢ/∂βₖ⁺ = pᵢ·(1-pᵢ)·xᵢₖ⁺**：
```
∂²LL/(∂βⱼ⁺∂βₖ⁺) = -Σᵢ₌₁ⁿ xᵢⱼ⁺ · pᵢ·(1-pᵢ) · xᵢₖ⁺
                  = -Σᵢ₌₁ⁿ pᵢ·(1-pᵢ) · xᵢⱼ⁺·xᵢₖ⁺
```

### 2.6 Hessian 矩阵的矩阵形式

**定义对角权重矩阵** W (n×n)：
```
W = diag(p₁(1-p₁), p₂(1-p₂), ..., pₙ(1-pₙ))
```

注意 W 的对角元素都是正数（因为 0 < pᵢ < 1）。

**Hessian 矩阵**：
```
H = ∂²LL/∂β⁺∂β⁺' = -X⁺' W X⁺
```

**验证**：
```
(X⁺' W X⁺)ⱼₖ = Σᵢ₌₁ⁿ (X⁺')ⱼᵢ · Wᵢᵢ · (X⁺)ᵢₖ
              = Σᵢ₌₁ⁿ xᵢⱼ⁺ · pᵢ(1-pᵢ) · xᵢₖ⁺  ✓
```

所以：
```
H = -X⁺' W X⁺
```

### 2.7 Hessian 是负定的（凸优化）

**证明 H 是负定的**：

对任意非零向量 v ∈ ℝᵖ⁺¹：
```
v' H v = -v' X⁺' W X⁺ v
       = -(X⁺v)' W (X⁺v)
       = -Σᵢ₌₁ⁿ pᵢ(1-pᵢ) · (X⁺v)ᵢ²
       = -Σᵢ₌₁ⁿ pᵢ(1-pᵢ) · (Σⱼ xᵢⱼ⁺vⱼ)²
```

因为：
- pᵢ(1-pᵢ) > 0 （对所有 0 < pᵢ < 1）
- (Σⱼ xᵢⱼ⁺vⱼ)² ≥ 0

所以 v' H v < 0（假设 X⁺ 列满秩，至少有一项 >0）

**结论**：H 是负定的 ⟹ -H 是正定的 ⟹ LL(β⁺) 是**严格凹函数**

这意味着：
1. 对数似然函数有**唯一的全局最大值**
2. 梯度上升/Newton法保证收敛到最优解
3. 没有局部最大值

### 2.8 Newton-Raphson 更新公式

**Newton法的标准形式**（最大化 f(θ)）：
```
θ_{new} = θ_{old} - [∂²f/∂θ²]⁻¹ · ∂f/∂θ
```

对于 LL(β⁺)：
```
β⁺_{t+1} = β⁺_t - H⁻¹ · ∂LL/∂β⁺
         = β⁺_t - (-X⁺'WX⁺)⁻¹ · X⁺'(y-p)
         = β⁺_t + (X⁺'WX⁺)⁻¹ · X⁺'(y-p)
```

### 2.9 总结 Logistic Regression Hessian

| 项目 | 公式 |
|------|------|
| **模型** | pᵢ = σ(β⁺'xᵢ⁺) = 1/(1+exp(-β⁺'xᵢ⁺)) |
| **对数似然** | LL = Σᵢ[yᵢlog(pᵢ) + (1-yᵢ)log(1-pᵢ)] |
| **梯度** | ∂LL/∂β⁺ = X⁺'(y-p) |
| **Hessian** | H = -X⁺'WX⁺ where W = diag(pᵢ(1-pᵢ)) |
| **性质** | H 负定 ⟹ LL 严格凹 ⟹ 唯一最优解 |
| **Newton更新** | β⁺_{t+1} = β⁺_t + (X⁺'WX⁺)⁻¹X⁺'(y-p) |

**关键公式**：
```
σ'(z) = σ(z)(1-σ(z))  ← sigmoid导数
∂pᵢ/∂βⱼ = pᵢ(1-pᵢ)xᵢⱼ  ← 概率对参数的导数
∂²LL/(∂βⱼ∂βₖ) = -Σᵢ pᵢ(1-pᵢ)xᵢⱼxᵢₖ  ← Hessian元素
```

---

## 推导 3: SVM 对偶问题

### 3.1 原始问题（Hard Margin SVM）

**目标**：找到最大间隔的分离超平面。

**原始优化问题**（Primal Problem）：
```
min_{w,b} ½‖w‖²

subject to: yᵢ(w'xᵢ + b) ≥ 1,  i = 1, 2, ..., n
```

其中：
- w ∈ ℝᵖ 是超平面的法向量
- b ∈ ℝ 是偏置项
- (xᵢ, yᵢ) 是训练数据，yᵢ ∈ {-1, +1}
- ‖w‖² = w'w

**几何意义**：
- 超平面方程：w'x + b = 0
- 点到超平面距离：|w'x + b|/‖w‖
- 间隔（margin）：2/‖w‖
- 最大化间隔 = 最小化 ‖w‖² = 最小化 ½‖w‖²

### 3.2 Lagrange 函数

**引入 Lagrange 乘子** αᵢ ≥ 0（每个约束一个）：

**Lagrange 函数**：
```
L(w, b, α) = ½‖w‖² - Σᵢ₌₁ⁿ αᵢ[yᵢ(w'xᵢ + b) - 1]
```

**展开**：
```
L(w, b, α) = ½w'w - Σᵢ₌₁ⁿ αᵢyᵢw'xᵢ - Σᵢ₌₁ⁿ αᵢyᵢb + Σᵢ₌₁ⁿ αᵢ
           = ½w'w - w'(Σᵢ₌₁ⁿ αᵢyᵢxᵢ) - b(Σᵢ₌₁ⁿ αᵢyᵢ) + Σᵢ₌₁ⁿ αᵢ
```

**原始问题等价于**：
```
min_{w,b} max_{α≥0} L(w, b, α)
```

### 3.3 对偶问题

根据**强对偶定理**（Strong Duality），在满足 Slater 条件下：
```
min_{w,b} max_{α≥0} L(w, b, α) = max_{α≥0} min_{w,b} L(w, b, α)
```

**SVM 满足 Slater 条件**（存在严格可行点），所以强对偶成立。

我们先求 min_{w,b} L(w, b, α)。

### 3.4 对 w 和 b 求偏导

**对 w 求偏导**：
```
∂L/∂w = w - Σᵢ₌₁ⁿ αᵢyᵢxᵢ = 0
```

**得到最优条件**：
```
w* = Σᵢ₌₁ⁿ αᵢyᵢxᵢ  ← 关键公式！
```

这表明 w 是训练样本的线性组合。

**对 b 求偏导**：
```
∂L/∂b = -Σᵢ₌₁ⁿ αᵢyᵢ = 0
```

**得到约束**：
```
Σᵢ₌₁ⁿ αᵢyᵢ = 0  ← 对偶问题的约束
```

### 3.5 代入得到对偶目标函数

将 w* = Σᵢ αᵢyᵢxᵢ 和 Σᵢ αᵢyᵢ = 0 代入 L(w, b, α)：

**第一项**：
```
½w'w = ½(Σᵢ αᵢyᵢxᵢ)'(Σⱼ αⱼyⱼxⱼ)
     = ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ
```

**第二项**：
```
w'(Σᵢ αᵢyᵢxᵢ) = (Σⱼ αⱼyⱼxⱼ)'(Σᵢ αᵢyᵢxᵢ)
                = ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ
```

**第三项**：
```
b(Σᵢ αᵢyᵢ) = 0  （因为 Σᵢ αᵢyᵢ = 0）
```

**第四项**：
```
Σᵢ αᵢ
```

**合并**：
```
L(w*, b, α) = ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ - ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ + Σᵢ αᵢ
            = -½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ + Σᵢ αᵢ
            = Σᵢ αᵢ - ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ
```

### 3.6 对偶问题的最终形式

**对偶问题**（Dual Problem）：
```
max_{α} Σᵢ₌₁ⁿ αᵢ - ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ

subject to:
    αᵢ ≥ 0,  i = 1, ..., n
    Σᵢ₌₁ⁿ αᵢyᵢ = 0
```

或者等价地写成**最小化形式**：
```
min_{α} ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼxᵢ'xⱼ - Σᵢ₌₁ⁿ αᵢ

subject to:
    αᵢ ≥ 0,  i = 1, ..., n
    Σᵢ₌₁ⁿ αᵢyᵢ = 0
```

### 3.7 引入核函数（Kernel Trick）

**定义内积** κ(xᵢ, xⱼ) = xᵢ'xⱼ

**对偶问题变为**：
```
max_{α} Σᵢ αᵢ - ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼκ(xᵢ, xⱼ)
```

**核技巧**：可以用任何核函数 κ(x, x') 替换内积，例如：

**线性核**：
```
κ(x, x') = x'x'
```

**多项式核**：
```
κ(x, x') = (x'x' + c)ᵈ
```

**RBF（高斯）核**：
```
κ(x, x') = exp(-γ‖x - x'‖²)
```

**核的意义**：
- 隐式地将数据映射到高维空间 φ(x)
- κ(x, x') = φ(x)'φ(x')
- 无需显式计算 φ(x)，只需计算核函数

### 3.8 KKT 条件

**Karush-Kuhn-Tucker (KKT) 条件**是约束优化的最优性必要条件。

对于 SVM，KKT 条件是：

**1. 原始可行性**：
```
yᵢ(w'xᵢ + b) ≥ 1,  ∀i
```

**2. 对偶可行性**：
```
αᵢ ≥ 0,  ∀i
```

**3. 互补松弛条件**（Complementary Slackness）：
```
αᵢ[yᵢ(w'xᵢ + b) - 1] = 0,  ∀i
```

**4. 平稳性条件**（已用过）：
```
w = Σᵢ αᵢyᵢxᵢ
Σᵢ αᵢyᵢ = 0
```

### 3.9 支持向量的定义

**互补松弛条件的含义**：
```
αᵢ[yᵢ(w'xᵢ + b) - 1] = 0
```

这意味着对每个样本 i，下面两种情况必有一种成立：

**情况 1**：αᵢ = 0
- 这个样本对决策边界没有贡献
- 它远离边界（yᵢ(w'xᵢ + b) > 1）

**情况 2**：yᵢ(w'xᵢ + b) = 1
- 这个样本恰好在边界上
- αᵢ > 0，这个样本被称为**支持向量**

**支持向量的重要性**：
- 只有支持向量的 αᵢ > 0
- w = Σᵢ αᵢyᵢxᵢ = Σ_{i∈SV} αᵢyᵢxᵢ （只有SV有贡献）
- 其他样本可以去掉而不影响结果

### 3.10 求解 b（偏置项）

**利用支持向量**：对任何支持向量 s（αₛ > 0）：
```
yₛ(w'xₛ + b) = 1
```

**解出 b**：
```
yₛw'xₛ + yₛb = 1
yₛb = 1 - yₛw'xₛ
```

因为 yₛ ∈ {-1, +1}，所以 yₛ² = 1：
```
b = yₛ - w'xₛ
  = yₛ - (Σᵢ αᵢyᵢxᵢ)'xₛ
  = yₛ - Σᵢ αᵢyᵢxᵢ'xₛ
```

**实际中用所有支持向量的平均**（更稳定）：
```
b = (1/|SV|) Σ_{s∈SV} [yₛ - Σᵢ αᵢyᵢxᵢ'xₛ]
```

### 3.11 决策函数

**训练后得到** α*（对偶问题的解）

**决策函数**：
```
f(x) = w'x + b
     = (Σᵢ αᵢyᵢxᵢ)'x + b
     = Σᵢ αᵢyᵢxᵢ'x + b
     = Σᵢ αᵢyᵢκ(xᵢ, x) + b  （用核函数）
```

**分类**：
```
ŷ = sign(f(x)) = sign(Σᵢ αᵢyᵢκ(xᵢ, x) + b)
```

**实际上只需要支持向量**：
```
ŷ = sign(Σ_{i∈SV} αᵢyᵢκ(xᵢ, x) + b)
```

### 3.12 Soft Margin SVM（软间隔）

对于线性不可分的情况，引入**松弛变量** ξᵢ ≥ 0：

**原始问题**：
```
min_{w,b,ξ} ½‖w‖² + C·Σᵢ ξᵢ

subject to:
    yᵢ(w'xᵢ + b) ≥ 1 - ξᵢ,  ∀i
    ξᵢ ≥ 0,  ∀i
```

**对偶问题**（推导类似）：
```
max_{α} Σᵢ αᵢ - ½ΣᵢΣⱼ αᵢαⱼyᵢyⱼκ(xᵢ, xⱼ)

subject to:
    0 ≤ αᵢ ≤ C,  ∀i          ← 唯一的变化！
    Σᵢ αᵢyᵢ = 0
```

**参数 C**：
- C 大：重视分类正确，间隔可能小（低 bias，高 variance）
- C 小：重视间隔大，允许一些误分类（高 bias，低 variance）

### 3.13 总结 SVM 对偶推导

| 步骤 | 关键公式 |
|------|---------|
| **1. 原始问题** | min ½‖w‖²,  s.t. yᵢ(w'xᵢ+b) ≥ 1 |
| **2. Lagrange函数** | L = ½‖w‖² - Σᵢαᵢ[yᵢ(w'xᵢ+b)-1] |
| **3. 对w求导** | w = Σᵢαᵢyᵢxᵢ |
| **4. 对b求导** | Σᵢαᵢyᵢ = 0 |
| **5. 代入L** | L = Σᵢαᵢ - ½ΣᵢΣⱼαᵢαⱼyᵢyⱼxᵢ'xⱼ |
| **6. 对偶问题** | max_{α} L,  s.t. αᵢ≥0, Σᵢαᵢyᵢ=0 |
| **7. 核技巧** | 用 κ(xᵢ,xⱼ) 替换 xᵢ'xⱼ |
| **8. KKT条件** | αᵢ[yᵢ(w'xᵢ+b)-1]=0 |
| **9. 支持向量** | αᵢ>0 的样本 |
| **10. 决策函数** | f(x) = Σᵢαᵢyᵢκ(xᵢ,x) + b |

**对偶的优势**：
1. 只依赖于内积 → 可以用核技巧
2. 样本数 n >> 特征数 p 时，对偶可能更高效
3. 稀疏解（只有支持向量的 α ≠ 0）

---

## 三个推导的总结对比

| 算法 | 核心思想 | 关键技巧 | 最优化问题 |
|------|---------|---------|-----------|
| **AdaBoost** | 指数损失 + 前向加法 | 权重重加权 | 贪心最小化损失 |
| **Logistic Regression** | 最大似然 | Sigmoid性质 | 凸优化（Newton法） |
| **SVM** | 最大间隔 | Lagrange对偶 + 核技巧 | 二次规划 |

希望这三个推导对你理解这些核心算法有帮助！每个推导都尽量做到了每一步清晰、不跳步骤。
