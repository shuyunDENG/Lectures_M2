# Machine Learning (SPLEX) - çŸ¥è¯†ç‚¹æ•´åˆä¸è€ƒæ ¸å†…å®¹

## åŸºäºER1è€ƒè¯•çš„æ ¸å¿ƒè€ƒç‚¹åˆ†æ

### ER1 2022-2023 è€ƒæ ¸å†…å®¹
1. **Spectral clustering and graph Laplacian** (3pts)
2. **Support Vector Machines (SVM)** (3pts)
3. **Linear classification/regression, Training vs Testing** (2pts)
4. **k-plus proches voisins (k-NN)** (3pts)
5. **Naive Bayes** (7pts)
6. **Perceptron** (4pts)

### ER1 2023-2024 è€ƒæ ¸å†…å®¹
1. **Short questions** - classifier selection, accuracy interpretation, regularization (3pts)
2. **Canonical Correlation Analysis (CCA)** (4pts)
3. **Gradient descent for k-means** (3pts)
4. **Contingency tables, probabilities, logistic regression, independence** (8pts)
5. **Decision trees with entropy** (5pts)

---

## Cours 1: Introduction, Evaluation et AgrÃ©gation de Classifieurs

### 1.1 æ ¸å¿ƒæ¦‚å¿µ

#### Classification Task (åˆ†ç±»ä»»åŠ¡å½¢å¼åŒ–)
- **Population** Î , **Descriptive space** D âŠ† â„^d, **Classes** C
- **Goal**: Find Äˆ: D â†’ C such that âˆ€Ï€, Äˆ(D(Ï€)) â‰ˆ C(Ï€)

#### Supervised Learning Methodology (ç›‘ç£å­¦ä¹ æ–¹æ³•è®º)
1. **Training** (Apprentissage): Learn Äˆ on Î â‚
2. **Validation**: Evaluate Äˆ on Î áµ¥
3. **Prediction**: Apply Äˆ on new data

### 1.2 å…³é”®æ¨å¯¼

#### Error Metrics (é”™è¯¯åº¦é‡)

**Erreur en gÃ©nÃ©ralisation** (æ³›åŒ–è¯¯å·®):
```
e(Äˆ) = ğ”¼â‚“[C(x) â‰  Äˆ(x)]
```

**Erreur en apprentissage**:
```
eâ‚(Äˆ) = (1/|Î â‚|) Î£_{xâˆˆÎ â‚} Î´(C(x) - Äˆ(x))
```

**Erreur en validation**:
```
eáµ¥(Äˆ) = (1/|Î áµ¥|) Î£_{xâˆˆÎ áµ¥} Î´(C(x) - Äˆ(x))
```

#### Confusion Matrix (æ··æ·†çŸ©é˜µ) åŠæŒ‡æ ‡

|  | C(x)=âŠ– | C(x)=âŠ• |
|---|---|---|
| **Äˆ(x)=âŠ–** | VN (True Negative) | FN (False Negative) |
| **Äˆ(x)=âŠ•** | FP (False Positive) | VP (True Positive) |

**å…³é”®æŒ‡æ ‡è®¡ç®—**:
- **Accuracy** (å‡†ç¡®ç‡): (VP + VN) / N
- **Precision** (ç²¾ç¡®ç‡): VP / (VP + FP)
- **Recall/Sensitivity** (å¬å›ç‡/çµæ•åº¦): VP / (VP + FN)
- **Specificity** (ç‰¹å¼‚æ€§): VN / (VN + FP)
- **F1-score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

#### Leave-One-Out Cross Validation (LOOCV)

**ç®—æ³•**:
```
For each Ï€ âˆˆ Î â‚:
    1. Train Äˆâ‚‹Ï€ on Î â‚ \ {Ï€}
    2. Calculate eÏ€ = |Äˆâ‚‹Ï€(Ï€) - C(Ï€)|

eLOOCV(Äˆ) = (1/|Î â‚|) Î£_{Ï€âˆˆÎ â‚} eÏ€
```

### 1.3 ROC Curve (ROCæ›²çº¿)

#### æ„é€ æ–¹æ³•
1. Sort data by g(x) values (classifier score)
2. For k = 0 to |Î â‚|, define Äˆâ‚– that classifies âŠ– for rank â‰¤ k
3. Plot (1-Specificity, Sensitivity) for each Äˆâ‚–

#### AUC (Area Under Curve)
- **Random classifier**: AUC = 0.5
- **Perfect classifier**: AUC = 1.0
- **Interpretation**: Probability that classifier ranks a positive higher than a negative

### 1.4 Bootstrap ä¸èšåˆæ–¹æ³•

#### BootstrapåŸç†
```
For b = 1 to B:
    1. Draw Lâ‚’ sample with replacement from L
    2. Estimate Î¸Ì‚â‚’ on sample Lâ‚’

LÌƒ = {Î¸Ì‚â‚, ..., Î¸Ì‚áµ¦}
```

**æ–¹å·®ä¼°è®¡**:
```
ÏƒÂ²Î¸Ì‚ â‰ˆ ÏƒÌ‚Â²áµ¦ = (1/(B-1)) Î£áµ¦(Î¸Ì‚â‚’ - (1/B)Î£áµ¦Î¸Ì‚â‚’)Â²
```

### 1.5 Bagging (Bootstrap Aggregating)

**ç®—æ³•**:
```
For i = 1 to I:
    1. Create bootstrap sample Î â½â±â¾â‚
    2. Train Äˆáµ¢ on Î â½â±â¾â‚

Äˆbagging(x) = arg max_{c} |{i: Äˆáµ¢(x) = c}|
```

**ç‰¹ç‚¹**: é™ä½æ–¹å·®ï¼Œå¯¹ä¸ç¨³å®šåˆ†ç±»å™¨æœ‰æ•ˆï¼ˆå¦‚å†³ç­–æ ‘ï¼‰

### 1.6 AdaBoost æ¨å¯¼

**æ ¸å¿ƒæ›´æ–°å…¬å¼**:

**Step t**:
1. Calculate error: Îµâ‚œ = eá´…â‚œ(Äˆâ‚œ) = Î£áµ¢ Dâ‚œ(xáµ¢) Â· ğŸ™[Äˆâ‚œ(xáµ¢) â‰  C(xáµ¢)]

2. Calculate Î²â‚œ:
```
Î²â‚œ = Îµâ‚œ / (1 - Îµâ‚œ)
```

3. Update weights:
```
Dâ‚œâ‚Šâ‚(xáµ¢) âˆ {
    Dâ‚œ(xáµ¢)      if Äˆâ‚œ(xáµ¢) = C(xáµ¢)
    Î²â‚œÂ·Dâ‚œ(xáµ¢)    if Äˆâ‚œ(xáµ¢) â‰  C(xáµ¢)
}
```

4. Calculate Î±â‚œ:
```
Î±â‚œ = (1/2) log(1/Î²â‚œ) = (1/2) log((1-Îµâ‚œ)/Îµâ‚œ)
```

**æœ€ç»ˆåˆ†ç±»å™¨**:
```
Äˆboosting(x) = sign(Î£â‚œ Î±â‚œÂ·Äˆâ‚œ(x))
```

**å…³é”®æ€§è´¨**: AdaBoostå¢åŠ marginï¼Œå‡å°‘biasï¼Œå³ä½¿è®­ç»ƒè¯¯å·®ä¸º0ä¹Ÿèƒ½ç»§ç»­æå‡æ³›åŒ–æ€§èƒ½

---

## Cours 2: Tests d'HypothÃ¨ses Statistiques

### 2.1 å‡è®¾æ£€éªŒæ¡†æ¶

#### Basic Setup
- **Hâ‚€**: Null hypothesis (å¾…æ£€éªŒå‡è®¾)
- **Hâ‚**: Alternative hypothesis (å¤‡æ‹©å‡è®¾)
- **Test statistic**: ç»Ÿè®¡é‡ç”¨äºå†³ç­–

#### Error Types (é”™è¯¯ç±»å‹)

|  | Hâ‚€ is true | Hâ‚ is true |
|---|---|---|
| **Reject Hâ‚€** | Type I error (Î±) | Correct |
| **Don't reject Hâ‚€** | Correct | Type II error (Î²) |

- **Î±**: Significance level (æ˜¾è‘—æ€§æ°´å¹³)
- **Power**: 1 - Î² (æ£€éªŒæ•ˆåŠ›)

### 2.2 Maximum Likelihood Estimation (MLE)

**Likelihood function**:
```
L(Î¸ : D) = P(D | Î¸) = Î â‚˜ P(xâ‚˜ | Î¸)
```

**MLE**:
```
Î¸Ì‚_MLE = arg max_Î¸ L(Î¸ : D)
```

**å¯¹äºäºŒé¡¹åˆ†å¸ƒ**:
```
L(Î¸ : D) = Î¸áµ–(1-Î¸)áµ  where p+q=n

dL/dÎ¸ = 0 âŸ¹ Î¸Ì‚ = p/(p+q)
```

### 2.3 Neyman-Pearson Lemma (ç®€å•å‡è®¾æœ€ä¼˜æ£€éªŒ)

For simple hypotheses Hâ‚€: Î¸ = Î¸â‚€ vs Hâ‚: Î¸ = Î¸â‚:

**Likelihood ratio test**:
```
Î»(x) = L(x, Î¸â‚€)/L(x, Î¸â‚)

Decision:
  Î»(x) > k  âŸ¹  Accept Hâ‚€
  Î»(x) < k  âŸ¹  Reject Hâ‚€
  Î»(x) = k  âŸ¹  Accept Hâ‚€ with probability Ï
```

k and Ï are determined by desired Î± level.

### 2.4 Ï‡Â² Distribution (å¡æ–¹åˆ†å¸ƒ)

**å®šä¹‰**: è‹¥ Xáµ¢ ~ N(0, 1) i.i.d., åˆ™:
```
Î£áµ¢â‚Œâ‚Ê³ Xáµ¢Â² ~ Ï‡Â²(r)
```

**Properties**:
- Mean = r (degrees of freedom)
- Variance = 2r
- For r > 100: Ï‡Â²(r) â‰ˆ N(r, 2r)

#### Corrected Variance Distribution

è‹¥ Xáµ¢ ~ N(Î¼, ÏƒÂ²), åˆ™:
```
SÂ² = Î£áµ¢(Xáµ¢ - XÌ„)Â² / (n-1)

(n-1)SÂ²/ÏƒÂ² ~ Ï‡Â²(n-1)
```

### 2.5 Ï‡Â² Goodness-of-Fit Test (æ‹Ÿåˆä¼˜åº¦æ£€éªŒ)

**Test statistic**:
```
DÂ²(n) = Î£â‚—â‚Œâ‚áµ (Nâ‚— - nÂ·pâ‚—)Â² / (nÂ·pâ‚—)
```

where:
- Nâ‚— = observed count in class l
- nÂ·pâ‚— = expected count under Hâ‚€
- k = number of classes

**Distribution**: DÂ²(n) ~ Ï‡Â²(k-1) when n â†’ âˆ

**Decision rule**: Reject Hâ‚€ if DÂ² > Ï‡Â²_{k-1,Î±}

### 2.6 Ï‡Â² Independence Test (ç‹¬ç«‹æ€§æ£€éªŒ)

**Contingency table** (åˆ—è”è¡¨):

|  | Bâ‚ | Bâ‚‚ | ... | Bâ±¼ | Total |
|---|---|---|---|---|---|
| Aâ‚ | nâ‚â‚ | nâ‚â‚‚ | ... | nâ‚â±¼ | nâ‚Â· |
| Aâ‚‚ | nâ‚‚â‚ | nâ‚‚â‚‚ | ... | nâ‚‚â±¼ | nâ‚‚Â· |
| ... | ... | ... | ... | ... | ... |
| Aáµ¢ | náµ¢â‚ | náµ¢â‚‚ | ... | náµ¢â±¼ | náµ¢Â· |
| Total | nÂ·â‚ | nÂ·â‚‚ | ... | nÂ·â±¼ | n |

**Under independence**:
```
E[náµ¢â±¼] = (náµ¢Â· Ã— nÂ·â±¼) / n
```

**Test statistic**:
```
Ï‡Â² = Î£áµ¢Î£â±¼ (náµ¢â±¼ - náµ¢Â·nÂ·â±¼/n)Â² / (náµ¢Â·nÂ·â±¼/n)
```

**Distribution**: Ï‡Â² ~ Ï‡Â²((I-1)Ã—(J-1))

### 2.7 Student's t-Distribution

**Definition**: For X ~ N(Î¼, ÏƒÂ²), sample size n:
```
T = (XÌ„ - Î¼)/(S/âˆšn) ~ t(n-1)
```

**Properties**:
- E[T] = 0
- Var(T) = n/(n-2) for n > 2
- As n â†’ âˆ, t(n) â†’ N(0,1)

#### Confidence Interval for Î¼

**ÏƒÂ² known, large n**:
```
CI = [xÌ„ Â± z_{Î±/2} Ã— Ïƒ/âˆšn]
```

**ÏƒÂ² unknown, X ~ N**:
```
CI = [xÌ„ Â± t_{n-1,Î±/2} Ã— s/âˆšn]
```

### 2.8 Two-Sample t-Test (ä¸¤æ ·æœ¬æ¯”è¾ƒ)

**Setup**:
- Sample 1: nâ‚, XÌ„â‚, sâ‚Â²
- Sample 2: nâ‚‚, XÌ„â‚‚, sâ‚‚Â²

**Hâ‚€**: Î¼â‚ = Î¼â‚‚

**Pooled variance**:
```
sÂ² = [(nâ‚-1)sâ‚Â² + (nâ‚‚-1)sâ‚‚Â²] / (nâ‚+nâ‚‚-2)
```

**Test statistic**:
```
t = (XÌ„â‚ - XÌ„â‚‚) / âˆš[sÂ²(1/nâ‚ + 1/nâ‚‚)] ~ t(nâ‚+nâ‚‚-2)
```

---

## Cours 3: Classification Non-paramÃ©trique

### 3.1 K-means Clustering

#### Algorithm
```
1. Initialize k cluster centers (randomly or k-means++)
2. Repeat until convergence:
   a. Assign each point to nearest center
   b. Update centers as mean of assigned points
```

#### å…³é”®æ¨å¯¼

**Objective function** (Inertia):
```
I_G = Î£â‚–â‚Œâ‚á´· Î£_{xáµ¢âˆˆGâ‚–} dÂ²(xáµ¢, gâ‚–)
```

where gâ‚– = (1/|Gâ‚–|) Î£_{xáµ¢âˆˆGâ‚–} xáµ¢

**Minimization**:
```
âˆ‚Iâ‚–/âˆ‚gâ‚– = Î£_{xáµ¢âˆˆGâ‚–} 2(xáµ¢ - gâ‚–) = 0

âŸ¹ gâ‚– = (1/|Gâ‚–|) Î£_{xáµ¢âˆˆGâ‚–} xáµ¢
```

**Inter-cluster inertia** (è¦æœ€å¤§åŒ–):
```
I_X = Î£â‚–â‚Œâ‚á´· |Gâ‚–| Â· dÂ²(gâ‚–, g)
```

where g = (1/n) Î£áµ¢ xáµ¢ (global centroid)

#### Gradient Descent for K-means (ER2023é¢˜ç›®)

**Loss function**:
```
â„“ = Î£â±¼â‚Œâ‚á´· Î£_{xáµ¢âˆˆSâ±¼} â€–xáµ¢ - câ±¼â€–Â²
```

**Gradient**:
```
âˆ‚â„“/âˆ‚câ±¼ = Î£_{xáµ¢âˆˆSâ±¼} 2(câ±¼ - xáµ¢) = 2(|Sâ±¼|Â·câ±¼ - Î£_{xáµ¢âˆˆSâ±¼} xáµ¢)
```

**Update rule**:
```
câ±¼^(t+1) = câ±¼^(t) - Î·Â·âˆ‚â„“/âˆ‚câ±¼
         = câ±¼^(t) - 2Î·(|Sâ±¼|Â·câ±¼^(t) - Î£_{xáµ¢âˆˆSâ±¼} xáµ¢)
```

### 3.2 Hierarchical Clustering (åˆ†å±‚èšç±»)

#### Linkage Criteria (è·ç¦»åº¦é‡)

**Single linkage** (æœ€å°è·ç¦»):
```
D(Gâ‚, Gâ‚‚) = min_{xâˆˆGâ‚, yâˆˆGâ‚‚} d(x, y)
```

**Complete linkage** (æœ€å¤§è·ç¦»):
```
D(Gâ‚, Gâ‚‚) = max_{xâˆˆGâ‚, yâˆˆGâ‚‚} d(x, y)
```

**Average linkage** (å¹³å‡è·ç¦»):
```
D(Gâ‚, Gâ‚‚) = (1/(|Gâ‚|Â·|Gâ‚‚|)) Î£_{xâˆˆGâ‚, yâˆˆGâ‚‚} d(x, y)
```

**Ward's method** (Wardæ–¹æ³•):
```
D(Gâ‚, Gâ‚‚) = (|Gâ‚|Â·|Gâ‚‚|)/(|Gâ‚|+|Gâ‚‚|) Â· d(gâ‚, gâ‚‚)Â²
```

Wardæ–¹æ³•æœ€å°åŒ–intra-cluster inertia increase.

### 3.3 k-Nearest Neighbors (k-NN)

#### Classification Rule
```
Äˆ(x) = (1/|V(x)|) Î£_{xáµ¢âˆˆV(x)} yáµ¢
```

where V(x) = k nearest neighbors of x

#### è·ç¦»åº¦é‡

**Euclidean (Lâ‚‚)**:
```
d(a,b) = âˆš(Î£áµ¢(aáµ¢-báµ¢)Â²)
```

**Manhattan (Lâ‚)**:
```
d(a,b) = Î£áµ¢|aáµ¢-báµ¢|
```

**Minkowski (Lâ‚š)**:
```
d(a,b) = (Î£áµ¢|aáµ¢-báµ¢|áµ–)^(1/p)
```

#### Parzen Window (Kernel Density Estimation)

**General form**:
```
g(x) = Î£áµ¢â‚Œâ‚â¿ Î¦(d(x,xáµ¢)/h) Â· yáµ¢
```

**Gaussian kernel**:
```
Î¦(u) = (1/âˆš(2Ï€)) exp(-uÂ²/2)
```

### 3.4 Decision Trees (å†³ç­–æ ‘)

#### Impurity Measures (ä¸çº¯åº¦åº¦é‡)

**Error rate**:
```
Error(N) = 1 - max_c P(c|N)
```

**Gini index**:
```
Gini(N) = 1 - Î£_c P(c|N)Â²
```

**Entropy** (Shannon entropy):
```
Entropy(N) = -Î£_c P(c|N)Â·logâ‚‚(P(c|N))
```

#### Information Gain (ä¿¡æ¯å¢ç›Š)

**å®šä¹‰**:
```
Î”(N, V) = I(N) - Î£áµ¥ (|Ráµ¥|/|R|)Â·I(Náµ¥)
```

where:
- N = parent node
- V = splitting variable
- Náµ¥ = child node for value v
- I(Â·) = impurity measure

#### Gain Ratio (å¢ç›Šæ¯”ç‡)

To avoid bias towards high-cardinality features:

```
Î”_Ratio(N, V) = Î”(N, V) / H(V)

where H(V) = -Î£áµ¥ P(V=v)Â·logâ‚‚(P(V=v))
```

#### Entropy ç†è®ºåŸºç¡€

**Hartley information**:
```
H(n) = logâ‚‚(n) = -logâ‚‚(1/n)
```

**Shannon entropy** for p = (pâ‚, ..., pâ‚™):
```
h(pâ‚, ..., pâ‚™) = -Î£áµ¢ páµ¢Â·logâ‚‚(páµ¢)
```

**Properties**:
- H(1) = 0 (no uncertainty)
- H(2) = 1 (one bit)
- H(nÂ·m) = H(n) + H(m) (additivity)

#### MDL (Minimum Description Length)

```
MDL(T) = Î±Â·Size(T) + Î£_{fâˆˆleaves(T)} I(f)
```

å¹³è¡¡æ ‘çš„å¤æ‚åº¦å’Œå¶å­èŠ‚ç‚¹çš„ä¸çº¯åº¦ã€‚

---

## Cours 5: Classification Probabiliste et LinÃ©aire Binaire

### 5.1 Probabilistic Classification Framework

#### Bayes' Theorem
```
P(Y|X) = P(X|Y)Â·P(Y) / P(X)
```

#### Maximum A Posteriori (MAP)
```
y*_MAP = arg max_y P(y|x) = arg max_y P(x|y)Â·P(y)
```

#### Maximum Likelihood (ML)
```
y*_ML = arg max_y P(x|y)
```

### 5.2 Naive Bayes Classifier

#### ç‹¬ç«‹æ€§å‡è®¾
```
âˆ€kâ‰ l, Xâ‚– âŠ¥âŠ¥ Xâ‚— | Y
```

å³: P(X|Y) = Î â‚– P(Xâ‚–|Y)

#### åˆ†ç±»è§„åˆ™
```
y* = arg max_y P(y) Â· Î â‚–â‚Œâ‚áµˆ P(xâ‚–|y)
```

#### Gaussian Naive Bayes

è‹¥ P(Xâ‚–|Y=y) ~ N(Î¼â‚–,y, ÏƒÂ²â‚–,y):

```
P(xâ‚–|y) = (1/âˆš(2Ï€ÏƒÂ²â‚–,y)) exp(-(xâ‚–-Î¼â‚–,y)Â²/(2ÏƒÂ²â‚–,y))
```

**å‚æ•°ä¼°è®¡**:
```
Î¼â‚–,y = (1/ny) Î£_{i:yáµ¢=y} xáµ¢â‚–

ÏƒÂ²â‚–,y = (1/ny) Î£_{i:yáµ¢=y} (xáµ¢â‚– - Î¼â‚–,y)Â²
```

where ny = |{i: yáµ¢ = y}|

### 5.3 Linear Binary Classification (CLB)

#### General Form
```
Äˆ(x) = Ïƒ(w'Â·x + wâ‚€) = Ïƒ(Î£áµ¢ wáµ¢xáµ¢ + wâ‚€)
```

where Ïƒ is sign function:
```
Ïƒ(u) = {-1  if u < 0
        +1  if u â‰¥ 0
```

#### å‡ ä½•è§£é‡Š

**Hyperplane equation**: w'Â·x + wâ‚€ = 0

**Distance from x to hyperplane**:
```
r = |w'Â·x + wâ‚€| / â€–wâ€–
```

**Normal vector**: w (perpendicular to hyperplane)

### 5.4 Logistic Regression

#### Logit Function
```
logit(p) = log(p/(1-p)) = log(P(âŠ•|x)/P(âŠ–|x))
```

#### Model
```
log(P(âŠ•|x)/P(âŠ–|x)) = w'Â·x + wâ‚€

âŸ¹ P(âŠ•|x) = exp(w'Â·x+wâ‚€)/(1+exp(w'Â·x+wâ‚€))

âŸ¹ P(âŠ–|x) = 1/(1+exp(w'Â·x+wâ‚€))
```

#### Log-Likelihood

For dataset (X, Y) with yáµ¢ âˆˆ {0,1}:

```
LL(Î²âº) = Î£áµ¢ [yáµ¢Â·(Î²'Â·xáµ¢âº) - log(1 + exp(Î²'Â·xáµ¢âº))]
```

where Î²âº = (Î², Î²â‚€), xáµ¢âº = (xáµ¢, 1)

#### Gradient
```
âˆ‚LL/âˆ‚Î²âº = Î£áµ¢ xáµ¢Â·(yáµ¢ - p(xáµ¢; Î²âº))
```

where p(xáµ¢; Î²âº) = exp(Î²'Â·xáµ¢âº)/(1+exp(Î²'Â·xáµ¢âº))

#### Newton-Raphson Update
```
Î²âº_{t+1} = Î²âº_t - [âˆ‚Â²LL/âˆ‚Î²âºâˆ‚Î²âº']â»Â¹ Â· âˆ‚LL/âˆ‚Î²âº
```

### 5.5 Fisher's Linear Discriminant

#### Between-class separation
```
Î”w = w'Â·(MâŠ• - MâŠ–)
```

where Mâ‚– = (1/nâ‚–) Î£_{iâˆˆk} xáµ¢

#### Within-class variance
```
sâ‚– = Î£_{iâˆˆk} (yáµ¢ - w'Â·Mâ‚–)Â²
```

where yáµ¢ = w'Â·xáµ¢

**Objective**: Maximize Î”wÂ² / (sâŠ• + sâŠ–)

### 5.6 Gaussian Discriminant Analysis

è‹¥ P(x|c) ~ N(Î¼c, Î£c):

**Density**:
```
p(x|c) = (1/((2Ï€)^(d/2)|Î£c|^(1/2))) exp(-Â½(x-Î¼c)'Î£câ»Â¹(x-Î¼c))
```

#### Linear Discriminant (Homoscedastic, Î£âŠ• = Î£âŠ– = Î£)

```
g(x) = (Î¼âŠ• - Î¼âŠ–)'Î£â»Â¹(x - xâ‚€)
```

where:
```
xâ‚€ = Â½(Î¼âŠ• + Î¼âŠ–) + [log(P(âŠ•)/P(âŠ–)) / ((Î¼âŠ•-Î¼âŠ–)'Î£â»Â¹(Î¼âŠ•-Î¼âŠ–))] Â· (Î¼âŠ• - Î¼âŠ–)
```

---

## Spectral Clustering (è°±èšç±»)

### åŸºæœ¬æ¦‚å¿µ

#### Graph Representation
- **G = (V, E)**: undirected graph
- **W**: weighted adjacency matrix (wáµ¢â±¼ â‰¥ 0)
- **D**: degree matrix (diagonal, dáµ¢ = Î£â±¼ wáµ¢â±¼)

#### Similarity Graphs

**Îµ-neighborhood**:
```
wáµ¢â±¼ = {1  if â€–xáµ¢-xâ±¼â€– < Îµ
       0  otherwise
```

**k-nearest neighbors**: Connect if xâ±¼ among k-NN of xáµ¢

**Fully connected with Gaussian kernel**:
```
wáµ¢â±¼ = exp(-â€–xáµ¢-xâ±¼â€–Â²/(2ÏƒÂ²))
```

### Graph Laplacian

#### Unnormalized Laplacian
```
L = D - W
```

**Properties**:
1. Symmetric and positive semi-definite
2. Smallest eigenvalue Î»â‚ = 0, eigenvector = ğŸ™ (all ones)
3. For any f âˆˆ â„â¿:
   ```
   f'Lf = Â½ Î£áµ¢â±¼ wáµ¢â±¼(fáµ¢ - fâ±¼)Â²
   ```

#### Normalized Laplacian
```
L_sym = Dâ»Â¹/Â²(D - W)Dâ»Â¹/Â² = I - Dâ»Â¹/Â²WDâ»Â¹/Â²
```

or:
```
L_rw = Dâ»Â¹(D - W) = I - Dâ»Â¹W
```

### Spectral Clustering Algorithm

**Unnormalized version**:
```
1. Compute adjacency matrix W
2. Compute Laplacian L = D - W
3. Compute first k eigenvectors vâ‚,...,vâ‚– of L
4. Form matrix V = [vâ‚ | ... | vâ‚–] âˆˆ â„â¿Ë£áµ
5. Treat each row of V as point yáµ¢ âˆˆ â„áµ
6. Run k-means on {yâ‚,...,yâ‚™}
```

**Normalized version (Ng-Jordan-Weiss)**:
```
1. Compute adjacency matrix W
2. Compute normalized Laplacian L_sym
3. Compute first k eigenvectors vâ‚,...,vâ‚– of L_sym
4. Form matrix V âˆˆ â„â¿Ë£áµ
5. Normalize rows: uáµ¢â±¼ = váµ¢â±¼ / (Î£â‚– váµ¢â‚–Â²)^(1/2)
6. Run k-means on rows of U
```

### Silhouette Score (è½®å»“ç³»æ•°)

**For point i**:
```
a(i) = (1/(nâ‚-1)) Î£_{jâˆˆCâ‚,jâ‰ i} d(i,j)  (average within-cluster distance)

b(i) = min_{kâ‰ a} (1/nâ‚–) Î£_{jâˆˆCâ‚–} d(i,j)  (distance to nearest cluster)

s(i) = (b(i) - a(i)) / max{a(i), b(i)}
```

**Interpretation**:
- s(i) â†’ +1: well clustered
- s(i) â‰ˆ 0: on cluster boundary
- s(i) â†’ -1: possibly misclassified

**Average for cluster k**:
```
sÌ„â‚– = (1/nâ‚–) Î£_{iâˆˆCâ‚–} s(i)
```

**Global average**:
```
Câ‚– = (1/n) Î£â‚– nâ‚–Â·sÌ„â‚–
```

---

## ç‰¹æ®Šä¸»é¢˜: Canonical Correlation Analysis (CCA)

### å®šä¹‰ä¸ç›®æ ‡

ç»™å®šä¸¤ç»„å˜é‡:
- **X = (Xâ‚,...,Xâ‚š)**: ç¬¬ä¸€ç»„å˜é‡
- **Y = (Yâ‚,...,Yáµ§)**: ç¬¬äºŒç»„å˜é‡

**Goal**: Find linear combinations:
```
U = a'X
V = b'Y
```

that maximize correlation Ï = corr(U, V)

### Canonical Correlation

**First canonical correlation**:
```
Ïâ‚ = max_{a,b} corr(a'X, b'Y)
```

subject to Var(a'X) = Var(b'Y) = 1

**Subsequent canonical correlations**: orthogonal to previous ones

**Properties**:
- Ï âˆˆ [-1, 1]
- Number of canonical correlations = min(p, q)

### Interpretation

**Weights (a, b)**: indicate importance of each variable in the relationship

**Unsupervised method**: explores relationships between variable groups without class labels

---

## Support Vector Machines (SVM) - ER2022è€ƒç‚¹

### Linear SVM (Hard Margin)

#### Primal Problem
```
min_{w,wâ‚€} Â½â€–wâ€–Â²

subject to: yáµ¢(w'xáµ¢ + wâ‚€) â‰¥ 1, âˆ€i
```

**Geometric interpretation**:
- Maximize margin 2/â€–wâ€–
- Support vectors: points where yáµ¢(w'xáµ¢ + wâ‚€) = 1

#### Decision Boundary
```
Äˆ(x) = sign(w'x + wâ‚€)
```

**Margin**:
```
M = 2/â€–wâ€–
```

### Soft Margin SVM

**Primal with slack variables**:
```
min_{w,wâ‚€,Î¾} Â½â€–wâ€–Â² + CÂ·Î£áµ¢Î¾áµ¢

subject to:
  yáµ¢(w'xáµ¢ + wâ‚€) â‰¥ 1 - Î¾áµ¢
  Î¾áµ¢ â‰¥ 0
```

**Parameter C**: trade-off between margin and misclassification

---

## Perceptron - ER2022è€ƒç‚¹

### Algorithm
```
Initialize: wâ½â°â¾, wâ‚€â½â°â¾
For each epoch:
  For each (xáµ¢, yáµ¢):
    if yáµ¢(w'xáµ¢ + wâ‚€) â‰¤ 0:  // misclassified
      w â† w + Î·Â·yáµ¢Â·xáµ¢
      wâ‚€ â† wâ‚€ + Î·Â·yáµ¢
```

**Learning rate**: Î· (often set to 1)

**Convergence**: Guaranteed if data is linearly separable

### Example Calculation (ER2022)

Given:
- Decision boundary: y = 0.5
- x-axis: Xâ‚, y-axis: Xâ‚‚
- Error: Â½

**Perceptron weights** for y = 0.5:
```
w'x + wâ‚€ = 0
If boundary is y = 0.5: -wâ‚‚/wâ‚ = slope, -wâ‚€/wâ‚ = intercept

For horizontal line y = 0.5:
wâ‚ = 0, wâ‚‚ = 1, wâ‚€ = -0.5
```

**Update with (wâ‚€,wâ‚,wâ‚‚) = (1,0,-2)**:
```
Point misclassified with Îµ = 1:
w â† w + ÎµÂ·yÂ·x
```

---

## è€ƒè¯•é¢˜å‹æ€»ç»“

### 1. æ¦‚å¿µæ€§é—®é¢˜
- é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨ï¼ˆæ¦‚ç‡ä¼°è®¡ â†’ probabilistic classifierï¼‰
- åˆ¤æ–­åˆ†ç±»å™¨æ€§èƒ½ï¼ˆimbalanced data â†’ check baselineï¼‰
- æ­£åˆ™åŒ–æ•ˆæœï¼ˆall coefficients = 0 â†’ Î»å¤ªå¤§ï¼‰

### 2. æ¨å¯¼é¢˜
- **k-means gradient descent**: âˆ‚â„“/âˆ‚câ±¼, update rule
- **Logistic regression**: likelihood, gradient, Newton-Raphson
- **Perceptron**: weight updates, convergence

### 3. æ¦‚ç‡è®¡ç®—é¢˜
- **Naive Bayes**: P(X,Y), P(Y), P(X|Y), decision rules
- **Contingency tables**: probabilities, independence tests
- **Logistic regression**: coefficients from contingency table

### 4. å›¾è®º/èšç±»é¢˜
- **Spectral clustering**: adjacency matrix, degree matrix, Laplacian, eigenvectors
- **Graph properties**: connectivity, similarity measures

### 5. å†³ç­–æ ‘é¢˜
- **Entropy calculation**: H(N) = -Î£ pÂ·logâ‚‚(p)
- **Information gain**: Î”(N,V)
- **Optimal tree**: compare gains

---

## é‡è¦å…¬å¼é€ŸæŸ¥

### Probability & Statistics
```
Bayes: P(Y|X) = P(X|Y)P(Y)/P(X)
Entropy: H(X) = -Î£ p(x)logâ‚‚p(x)
Ï‡Â² test: Ï‡Â² = Î£(O-E)Â²/E
t-test: t = (xÌ„-Î¼)/(s/âˆšn)
```

### Clustering
```
K-means objective: Î£â‚– Î£_{xâˆˆCâ‚–} â€–x-Î¼â‚–â€–Â²
Silhouette: s(i) = (b(i)-a(i))/max{a(i),b(i)}
Laplacian: L = D - W
```

### Classification
```
Logistic: P(y=1|x) = 1/(1+exp(-w'x))
Perceptron update: w â† w + Î·Â·yÂ·x (if error)
SVM margin: M = 2/â€–wâ€–
```

### Evaluation
```
Accuracy: (TP+TN)/(TP+TN+FP+FN)
Precision: TP/(TP+FP)
Recall: TP/(TP+FN)
F1: 2Â·PrecisionÂ·Recall/(Precision+Recall)
```

---

## å­¦ä¹ å»ºè®®

1. **æŒæ¡æ¨å¯¼**: ç‰¹åˆ«æ˜¯logistic regression, k-means gradient, information gain
2. **ç†è§£æ¦‚å¿µ**: Naive Bayeså‡è®¾, spectral clusteringåŸç†, SVM margin
3. **ç†Ÿç»ƒè®¡ç®—**: æ¦‚ç‡è¡¨, æ··æ·†çŸ©é˜µ, entropy, Ï‡Â²ç»Ÿè®¡é‡
4. **å®è·µåº”ç”¨**: çŸ¥é“ä½•æ—¶ç”¨å“ªä¸ªç®—æ³•ï¼Œç†è§£trade-offs
5. **å¤ä¹ ERé¢˜**: ä¸¤ä»½ERè¦†ç›–äº†å¤§éƒ¨åˆ†è€ƒç‚¹

ç¥è€ƒè¯•é¡ºåˆ©ï¼
